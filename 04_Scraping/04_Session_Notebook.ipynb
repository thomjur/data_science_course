{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0697f2f",
   "metadata": {},
   "source": [
    "# Web Scraping (08.11.2022)\n",
    "by Thomas Jurczyk (Dr. Eberle Zentrum, Universität Tübingen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d189aa62",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77dab70",
   "metadata": {},
   "source": [
    "1. What is web scraping? Why is it important for data science?\n",
    "2. Prerequisites\n",
    "3. Ethical/legal questions\n",
    "4. Web scraping tools in Python\n",
    "5. Example / Hands-On"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fae0fc",
   "metadata": {},
   "source": [
    "# What is web scraping?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55ad45d",
   "metadata": {},
   "source": [
    "Web scraping simply means collecting data from websites (in an automated way). Oftentimes, this means parsing the HTML tree of a website to get to the information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cafac54",
   "metadata": {},
   "source": [
    "For instance, you might want to check the titles of a News website on a daily basis. In order to do so, you could set up a web scraper that check the titles on a daily basis and writes them to a CSV file. Note, however, that you need to be aware of legal and ethical aspects. We will talk about that in a moment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa318183",
   "metadata": {},
   "source": [
    "Instead of checking a single website for information, you can also set up a so-called **web crawler** that jumps from website to website. For instance, you could check if a website contains a title field, then parse the title, check if the website contains any external links and follow these links to check if the linked websites also include titles, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec2a0ce",
   "metadata": {},
   "source": [
    "## Static vs. dynamic websites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77244cc6",
   "metadata": {},
   "source": [
    "**Static** websites are websites written in HTML that are served by a web server to every user in the same way. This means that whenever you request this site via an URL, you will receive the same website. On the other hand, **dynamic websites** are dynamically loaded depending on the user, time of the day, etc. A good example are shopping websites. The main page of a shopping website might look different to you than to any other user because certain products might appear based don your previous purchases, you might see different adds, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3830243e",
   "metadata": {},
   "source": [
    "In this course, we will only be dealing with static websites. Even though the structure of a static website might also change (for instance, because it has been replaced by a different version), it is usually much easier to scrape static websites than dynamic websites."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6744f2d",
   "metadata": {},
   "source": [
    "Sometimes, it is not even possible to scrape dynamic websites at all, at least with the tools we are using here. For example, some websites only serve some JavaScript code which is then executed by the browser. If we were to scrape such a website, we would only receive the JavaScript code but not the content we are interested in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92aa3e21",
   "metadata": {},
   "source": [
    "# Why is it important for data science?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bbd071",
   "metadata": {},
   "source": [
    "Overall, web scraping should only be the second best option if you want to collect data from the web. There are simply too many pitfalls:\n",
    "\n",
    "* legal/ethical issues\n",
    "* mal-formatted HTML\n",
    "* dynamic websites/JavaScript\n",
    "* changing HTML tree structures of the websites you are interested in\n",
    "* etc.\n",
    "\n",
    "If there is an API, always use the API. If there is no API, make sure that you are allowed to acquire the data and try to program a web scraper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef94bfd8",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0082b095",
   "metadata": {},
   "source": [
    "# Ethical/Legal questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02835cc",
   "metadata": {},
   "source": [
    "# Web scraping tools in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702fd2fe",
   "metadata": {},
   "source": [
    "## Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbf0e59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fe43dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35b6c502",
   "metadata": {},
   "source": [
    "## Beautifulsoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595c6f8a",
   "metadata": {},
   "source": [
    "# Example / Hands-On"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1658ca4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "715248a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEBSITE = \"https://tjurczyk.de/scraping/info.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90c91cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(WEBSITE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1e159cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59e2b2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d13dad7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = soup.find_all(\"p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b9d31336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a href=\"https://www.projekt-gutenberg.org/sinsheim/shylock/shylock.html\">Projekt Gutenberg</a>]\n",
      "[<a href=\"https://www.merriam-webster.com/\" target=\"_blank\">Meriam Webster</a>]\n"
     ]
    }
   ],
   "source": [
    "for res in results:\n",
    "    link = res.find_all(\"a\")\n",
    "    if link:\n",
    "        print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c4564cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"para-text\">Ich habe dieses Buch in den Jahren 1936 und 1937 geschrieben – in einer Welt, die es heute\n",
       "         nicht mehr gibt: in der ehemaligen Hauptstadt des Deutschen Reiches, Berlin, in der Welt der Nazis, der\n",
       "         Konzentrationslager und Pogrome, der Folterungen und Morde. Kein Wunder, daß dieses Buch, wenn es jetzt\n",
       "         erscheint, bereits eine Geschichte hinter sich hat, die zu erzählen sich lohnt.</p>,\n",
       " <p class=\"para-text\">Ich hatte gerade einen zwei Jahre währenden Kampf um die Befreiung eines älteren Bruders hinter\n",
       "         mir, den das Nazi-Regime wegen angeblichen Landesverrats eingesperrt hatte. Der von den Nazis im April 1934\n",
       "         eingesetzte Volksgerichtshof üblen Angedenkens sprach meinen Bruder so gut wie frei. Ja, jener Volksgerichtshof\n",
       "         tat sogar das Seine, um ihn vor der Gestapo zu bewahren – am 22. Dezember 1935! Ich werde diesen Tag so wie die\n",
       "         vorausgegangenen zwei Jahre des Kampfes nie vergessen. <a href=\"https://www.merriam-webster.com/\" target=\"_blank\">Meriam Webster</a></p>,\n",
       " <p class=\"para-text\">Jedes bedeutende Kunstwerk hat auch geschichtliche Bedeutung. Es verzeichnet Geschichte und hat\n",
       "         teil an der Geschichte. Shakespeares »Kaufmann von Venedig« aus diesem Blickwinkel zu betrachten, ist die\n",
       "         Absicht dieses Buches. Shakespeare hat die ›haltbarste‹ nachbiblische Judenfigur geschaffen. Er hat in ihr\n",
       "         notwendig über das Judentum berichtet und gerichtet. Er hat mit ihr jüdische Geschichte geschrieben und gemacht.\n",
       "     </p>,\n",
       " <p class=\"para-text\">Dieses Buch will ihm, vom jüdischen Standpunkt aus, den Zoll dafür erstatten, indem es die\n",
       "         Geschichte seines Shylock schreibt und erklärt. Indem es diese Geschichte schreibt, muß es den Shylock nicht nur\n",
       "         literarisch, sondern auch historisch und mythologisch zu ergründen suchen. Ihr Spielraum ist das 16.\n",
       "         Jahrhundert. Ihr Lebensraum aber ist das Schicksal des jüdischen Volkes von der biblischen Zeit an bis heute.\n",
       "     </p>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.select(\".para-text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41ff05a",
   "metadata": {},
   "source": [
    "# Literature"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
